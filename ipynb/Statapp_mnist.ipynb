{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statapp 2015 : Apprentissage MNIST"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Commen\u00e7ons par importer la base, peut-\u00eatre il vaudrait mieux l'enregistrer sur le serveur, parce que le download est un peu long !\n",
      "\n",
      "(Pour faire \u00e7a plus facilement: http://scikit-learn.org/stable/datasets/#downloading-datasets-from-the-mldata-org-repository)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "t = time.time()\n",
      "url = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
      "\n",
      "try : \n",
      "    print ('Try')\n",
      "    import gzip\n",
      "    mnist_dataset = gzip.open(os.path.basename(url), 'rb')\n",
      "\n",
      "    import pickle\n",
      "    u = pickle.Unpickler(mnist_dataset)\n",
      "    u.encoding = 'latin1'\n",
      "    train_set, valid_set, test_set = u.load()\n",
      "\n",
      "    def shared_dataset(data_xy):\n",
      "\n",
      "        data_x, data_y = data_xy\n",
      "        return data_x, data_y\n",
      "\n",
      "    test_set_x, test_set_y = shared_dataset(test_set)\n",
      "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "    train_set_x, train_set_y = shared_dataset(train_set)\n",
      "    \n",
      "except :\n",
      "    print ('Except')\n",
      "\n",
      "    import urllib2\n",
      "    req = urllib2.urlopen (url)\n",
      "    print(\"downloading \" + url)\n",
      "    \n",
      "    import os\n",
      "    with open(os.path.basename(url), \"wb\") as local_file:\n",
      "            local_file.write(req.read())\n",
      "    \n",
      "    import gzip\n",
      "    mnist_dataset = gzip.open(os.path.basename(url), 'rb')\n",
      "\n",
      "    import pickle\n",
      "    u = pickle.Unpickler(mnist_dataset)\n",
      "    u.encoding = 'latin1'\n",
      "    train_set, valid_set, test_set = u.load()\n",
      "\n",
      "    def shared_dataset(data_xy):\n",
      "\n",
      "        data_x, data_y = data_xy\n",
      "        return data_x, data_y\n",
      "\n",
      "    test_set_x, test_set_y = shared_dataset(test_set)\n",
      "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "    train_set_x, train_set_y = shared_dataset(train_set)\n",
      "\n",
      "print (time.time() - t) #3.77 s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Regression logistique multinomiale"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "t= time.time()\n",
      "\n",
      "print(t)\n",
      "\n",
      "import numpy as np\n",
      "#from sklearn import datasets\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "from sklearn import grid_search\n",
      "from sklearn import linear_model\n",
      "import scipy\n",
      "\n",
      "possible_parameters = [0.3]\n",
      "print(possible_parameters)\n",
      "\n",
      "clf_rlm = grid_search.GridSearchCV(\n",
      "        linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
      "        fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None),\n",
      "        {'C': possible_parameters},\n",
      "        cv=3,\n",
      "        scoring='accuracy',\n",
      "        refit=True\n",
      "    )\n",
      "clf_rlm.fit(train_set_x, train_set_y)\n",
      "print(\"meilleur estimateur : \", clf_rlm.best_estimator_)\n",
      "print(\"meilleur parametre : \", clf_rlm.best_params_)\n",
      "print(\"score : \", clf_rlm.scorer_)\n",
      "print (\"final score: \", metrics.accuracy_score(test_set_y, clf_rlm.predict(test_set_x)))\n",
      "\n",
      "\n",
      "\n",
      "print (time.time()-t) #4min for one iteration #On the new server 308s #On the new version of the server : 310s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred_set_y_rlm = clf_rlm.fit(train_set_x, train_set_y).predict(test_set_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "cm = confusion_matrix(test_set_y, pred_set_y_rlm)\n",
      "\n",
      "print(cm)\n",
      "\n",
      "cm_ = np.zeros((10,10), dtype=np.float)\n",
      "for i in range (0, len(cm)):\n",
      "    sum_list = sum(cm[i])\n",
      "    for j in range (0, len(cm[i])) :\n",
      "        cm_[i][j] = (1.0 * cm[i][j]/ sum_list)*1000\n",
      "    \n",
      "print(cm_)\n",
      "    \n",
      "# Show confusion matrix in a separate window\n",
      "plt.matshow(cm_)\n",
      "plt.title('Confusion matrix (per thousand)')\n",
      "plt.colorbar()\n",
      "plt.ylabel('True label')\n",
      "plt.xlabel('Predicted label')\n",
      "plt.show()\n",
      "\n",
      "print (time.time()-t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Arbre de d\u00e9cision"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import sklearn as sk\n",
      "#from sklearn.decomposition import PCA\n",
      "#from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "#from sklearn.tree import export_graphviz\n",
      "#from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "#from sklearn.cross_validation import train_test_split\n",
      "#from numpy import array\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "from sklearn import grid_search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curves = []\n",
      "for max_depth in range(10,20) :\n",
      "    clf = DecisionTreeClassifier(min_samples_leaf=5, max_depth=max_depth)\n",
      "    clf = clf.fit(train_set_x, train_set_y)\n",
      "    erra = accuracy_score( clf.predict(train_set_x), train_set_y)\n",
      "    errb = accuracy_score( clf.predict(test_set_x), test_set_y)\n",
      "    print(\"max_depth\",max_depth, \"erreur\",erra,errb)\n",
      "    curves.append((max_depth, erra,errb, clf) )\n",
      "plt.plot ( [c[0] for c in curves], [c[1] for c in curves], label=\"train\")\n",
      "plt.plot ( [c[0] for c in curves], [c[2] for c in curves], label=\"test\")\n",
      "plt.ylim( [0.8,1] )\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curves = []\n",
      "for min_samples_leaf in range(1,30) :\n",
      "    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, max_depth=14)\n",
      "    clf = clf.fit(train_set_x, train_set_y)\n",
      "    erra = accuracy_score( clf.predict(train_set_x), train_set_y)\n",
      "    errb = accuracy_score( clf.predict(test_set_x), test_set_y)\n",
      "    print(\"min_samples_leaf\",min_samples_leaf, \"erreur\",erra,errb)\n",
      "    curves.append((min_samples_leaf, erra,errb, clf) )\n",
      "plt.plot ( [c[0] for c in curves], [c[1] for c in curves], label=\"train\")\n",
      "plt.plot ( [c[0] for c in curves], [c[2] for c in curves], label=\"test\")\n",
      "plt.ylim( [0.8,1] )\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "clf_ad = DecisionTreeClassifier(min_samples_leaf=5, max_depth=14)\n",
      "pred_set_y_ad = clf_ad.fit(train_set_x, train_set_y).predict(test_set_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "cm = confusion_matrix(test_set_y, pred_set_y_ad)\n",
      "\n",
      "print(cm)\n",
      "\n",
      "cm_ = np.zeros((10,10), dtype=np.float)\n",
      "for i in range (0, len(cm)):\n",
      "    sum_list = sum(cm[i])\n",
      "    for j in range (0, len(cm[i])) :\n",
      "        cm_[i][j] = (1.0 * cm[i][j]/ sum_list)*1000\n",
      "    \n",
      "print(cm_)\n",
      "    \n",
      "# Show confusion matrix in a separate window\n",
      "plt.matshow(cm_)\n",
      "plt.title('Confusion matrix (per thousand)')\n",
      "plt.colorbar()\n",
      "plt.ylabel('True label')\n",
      "plt.xlabel('Predicted label')\n",
      "plt.show()\n",
      "\n",
      "print (time.time()-t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Comparaison des deux algos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "delta = pred_set_y_rlm - pred_set_y_ad\n",
      "acc_ad = accuracy_score(clf_ad.predict(test_set_x), test_set_y)\n",
      "acc_rlm = accuracy_score(clf_rlm.predict(test_set_x), test_set_y)\n",
      "i= 0\n",
      "arr_delta = []\n",
      "for element in delta :\n",
      "    i+=1\n",
      "    if element != 0 :\n",
      "        arr_delta.append(i)\n",
      "print (len(arr_delta))\n",
      "\n",
      "best_ad = -10000+acc_ad*10000 + len(arr_delta)\n",
      "best_rlm = -10000+acc_rlm*10000 + len(arr_delta)\n",
      "best_comb = len(arr_delta)-best_ad-best_rlm\n",
      "print([acc_ad,best_ad], [acc_rlm,best_rlm], [(10000-best_comb)/10000,best_comb])\n",
      "#avec une bonne m\u00e9trique le max de perf : 93,38%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "curves = []\n",
      "for n_estimators in range(3,20) :\n",
      "    clf_rf = RandomForestClassifier(n_estimators=10*n_estimators,max_depth=14,min_samples_leaf=5)\n",
      "    clf_rf = clf_rf.fit(train_set_x, train_set_y)\n",
      "    erra = accuracy_score( clf_rf.predict(train_set_x), train_set_y)\n",
      "    errb = accuracy_score( clf_rf.predict(test_set_x), test_set_y)\n",
      "    print(\"n_estimators\",10*n_estimators, \"erreur\",erra,errb)\n",
      "    curves.append((10*n_estimators, erra,errb, clf_rf) )\n",
      "plt.plot ( [c[0] for c in curves], [c[1] for c in curves], label=\"train\")\n",
      "plt.plot ( [c[0] for c in curves], [c[2] for c in curves], label=\"test\")\n",
      "plt.ylim( [0.9,1] )\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "clf_rf = RandomForestClassifier(n_estimators=80,max_depth=14,min_samples_leaf=5)\n",
      "pred_set_y_rf = clf_rf.fit(train_set_x, train_set_y).predict(test_set_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "cm = confusion_matrix(test_set_y, pred_set_y_rf)\n",
      "\n",
      "print(cm)\n",
      "\n",
      "cm_ = np.zeros((10,10), dtype=np.float)\n",
      "for i in range (0, len(cm)):\n",
      "    sum_list = sum(cm[i])\n",
      "    for j in range (0, len(cm[i])) :\n",
      "        cm_[i][j] = (1.0 * cm[i][j]/ sum_list)*1000\n",
      "    \n",
      "print(cm_)\n",
      "    \n",
      "# Show confusion matrix in a separate window\n",
      "plt.matshow(cm_)\n",
      "plt.title('Confusion matrix (per thousand)')\n",
      "plt.colorbar()\n",
      "plt.ylabel('True label')\n",
      "plt.xlabel('Predicted label')\n",
      "plt.show()\n",
      "\n",
      "print (time.time()-t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}